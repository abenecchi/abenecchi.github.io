<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Appendix A: Code | Classification of fake news article titles using character-level convolutional neural networks</title>
  <meta name="description" content="Appendix A: Code | Classification of fake news article titles using character-level convolutional neural networks" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Appendix A: Code | Classification of fake news article titles using character-level convolutional neural networks" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Appendix A: Code | Classification of fake news article titles using character-level convolutional neural networks" />
  
  
  

<meta name="author" content="Andrew Benecchi" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="references.html"/>

<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="assets/header-attrs-2.11/header-attrs.js"></script>
<script src="assets/jquery-2.2.3/jquery.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="assets/kePrint-0.0.1/kePrint.js"></script>
<link href="assets/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#exploratory-data-analysis"><i class="fa fa-check"></i>Exploratory Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="literature-review.html"><a href="literature-review.html"><i class="fa fa-check"></i>Literature Review</a></li>
<li class="chapter" data-level="" data-path="methodology.html"><a href="methodology.html"><i class="fa fa-check"></i>Methodology</a>
<ul>
<li class="chapter" data-level="" data-path="methodology.html"><a href="methodology.html#computer-information"><i class="fa fa-check"></i>Computer Information</a></li>
<li class="chapter" data-level="" data-path="methodology.html"><a href="methodology.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="methodology.html"><a href="methodology.html#preprocessing"><i class="fa fa-check"></i>Preprocessing</a></li>
<li class="chapter" data-level="" data-path="methodology.html"><a href="methodology.html#classification"><i class="fa fa-check"></i>Classification</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i>Results</a>
<ul>
<li class="chapter" data-level="" data-path="results.html"><a href="results.html#conclusions-and-limitations"><i class="fa fa-check"></i>Conclusions and Limitations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="" data-path="appendix-a-code.html"><a href="appendix-a-code.html"><i class="fa fa-check"></i>Appendix A: Code</a>
<ul>
<li class="chapter" data-level="" data-path="appendix-a-code.html"><a href="appendix-a-code.html#required-packages"><i class="fa fa-check"></i>Required Packages</a></li>
<li class="chapter" data-level="" data-path="appendix-a-code.html"><a href="appendix-a-code.html#fake-news-code"><i class="fa fa-check"></i>Fake News Code</a></li>
<li class="chapter" data-level="" data-path="appendix-a-code.html"><a href="appendix-a-code.html#histograms"><i class="fa fa-check"></i>Histograms</a></li>
<li class="chapter" data-level="" data-path="appendix-a-code.html"><a href="appendix-a-code.html#models"><i class="fa fa-check"></i>Models</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Classification of fake news article titles using character-level convolutional neural networks</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="appendix-a-code" class="section level1 unnumbered">
<h1>Appendix A: Code</h1>
<div id="required-packages" class="section level3 unnumbered">
<h3>Required Packages</h3>
<pre><code>import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Input, Embedding, Activation, Flatten, Dense
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.utils import plot_model
import math
import time
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import re
import os
import os
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.python.client import device_lib
# Model saving
import bz2
import pickle
import _pickle as cPickle
# Saves the &quot;data&quot; with the &quot;title&quot; and adds the .pickle
def full_pickle(title, data):
    pikd = open(title + &#39;.pickle&#39;, &#39;wb&#39;)
    pickle.dump(data, pikd)
    pikd.close()
def loosen(file):
    pikd = open(file, &#39;rb&#39;)
    data = pickle.load(pikd)
    pikd.close()
    return data
# Pickle a file and then compress it into a file with extension 
def compressed_pickle(title, data):
    with bz2.BZ2File(title + &#39;.pbz2&#39;, &#39;w&#39;) as f: 
        cPickle.dump(data, f)
def decompress_pickle(file):
    data = bz2.BZ2File(file, &#39;rb&#39;)
    data = cPickle.load(data)
    return data
device_lib.list_local_devices()</code></pre>
</div>
<div id="fake-news-code" class="section level3 unnumbered">
<h3>Fake News Code</h3>
<pre><code>class fakeNews:
    def __init__(self,directory=&#39;/mnt/c/Users/thecu/Documents/R Projects/4270/charLevelCNN/&#39;,lowercase=True):
        self.lowercase = lowercase
        self.directory = directory
        if lowercase == True:
            self.alphabet =
            &quot;abcdefghijklmnopqrstuvwxyz0123456789,;.!?:&#39;\&quot;/\\|_@#$%^&amp;*~`+-=&lt;&gt;()[]{}&quot;
        else:
            self.alphabet =
            &quot;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:&#39;\&quot;/\\|_@#$%^&amp;*~`+-=&lt;&gt;()[]{}&quot;
    def make_fakeNews(self, balance=True, content = False, pad = 1.25):
        self.pad = pad
        self.balance = balance
        self.content = content
        self.pad = pad
        articles = []
        for dirname, _, filenames in os.walk(self.directory+&#39;data&#39;):
            for filename in filenames:
                mypath = os.path.join(dirname, filename)
                name = re.sub(&#39;\.csv&#39;,&#39;&#39;,filename + &#39;_all&#39;)

            exec(&quot;%s = pd.read_csv(&quot; % (name,)+
            &quot;&#39;{path}&quot;.format(path=mypath)+&quot;&#39;)&quot;)
            if name == &#39;Fake_all&#39;:
                booler = 1
            else:
                booler = 0
            exec(&quot;%s.insert(%s.shape[1],&#39;y&#39;,[%d]*%s.shape[0])&quot; % (name,name,booler,name))
            exec(&quot;articles.append(%s)&quot; % (name,))
    if self.balance == True:
        clsLen = []
        for df in articles:
            clsLen.append(df.shape[0])
        classSize = int(np.min(100*np.floor(np.array(clsLen)/100)))
        articles_balanced = []
        for df in articles:
            articles_balanced.append(df.iloc[:classSize,:])
        Articles_bal = pd.concat(articles_balanced,axis=0,ignore_index=True)
        Articles_all = Articles_bal
    else:
        Articles_ubl = pd.concat(articles,axis=0,ignore_index=True)
        Articles_all = Articles_ubl
    tk = Tokenizer(num_words=None, char_level=True, oov_token=&#39;UNK&#39;)
    train_titles = Articles_all.iloc[:,0]
    train_bodies = Articles_all.iloc[:,1]
    all_y = Articles_all[&#39;y&#39;].values
    tk.fit_on_texts(train_titles)
    char_dict = {}
    for i, char in enumerate(self.alphabet):
        char_dict[char] = i + 1

    # Use char_dict to replace the tk.word_index
    tk.word_index = char_dict.copy()
    # Add &#39;UNK&#39; to the vocabulary
    tk.word_index[tk.oov_token] = max(char_dict.values()) + 1
    # Convert string to index
    title_sequences = tk.texts_to_sequences(train_titles)
    max_ttl = int(math.ceil(self.pad*max([len(x) for x in Articles_all.iloc[:,0].values])))
    if self.content == True:
        content_sequences = tk.texts_to_sequences(train_bodies)
        max_cnt = int(math.ceil(self.pad*max([len(x) for x in Articles_all.iloc[:,1].values])))
        coded_titles = pad_sequences(title_sequences, maxlen=max_ttl,
        padding=&#39;post&#39;)
        coded_bodies = pad_sequences(content_sequences, maxlen=max_cnt,
        padding=&#39;post&#39;)
        return (np.concatenate([coded_titles,coded_bodies],axis=1).astype(&#39;int32&#39;),
        all_y,tk.word_index)
    else:  
        return (pad_sequences(title_sequences, maxlen=max_ttl, padding=&#39;post&#39;).astype(&#39;int32&#39;), all_y,tk.word_index)
def three_way_split(self,make_Fake, split=&#39;default&#39;):
    self.X = make_Fake[0]
    self.y = make_Fake[1]
    tot_len = self.y.shape[0]
    self.split = split
    if self.split == &#39;default&#39;:
        tts = (7,2,1)
    else:
        tts = self.split
    tsp = int((tts[2]*tot_len/20))
    vsp = int(((tts[1]+tts[2])*tot_len/20))
    hlf = int((1*tot_len/2))
    test_tp =  (np.concatenate([self.X[:tsp,:],self.X[hlf:(hlf+tsp),:]],axis=0), 
    np.concatenate([self.y[:tsp],self.y[hlf:(hlf+tsp)]],axis=0))
    valid_tp = (np.concatenate([self.X[tsp:vsp,:],self.X[(hlf+tsp):(hlf+vsp),:]],axis=0),
    np.concatenate([self.y[tsp:vsp], self.y[(hlf+tsp):(hlf+vsp)]],axis=0))
    train_tp = (np.concatenate([self.X[vsp:hlf,:],self.X[(hlf+vsp):,:]],axis=0),
    np.concatenate([self.y[vsp:hlf],self.y[(hlf+vsp):]],axis=0)) 
    app = []
    for tp in [train_tp, valid_tp,test_tp]:
        alength = np.arange(tp[1].shape[0])
        np.random.seed(5318008)
        np.random.shuffle(alength)
        wee = []
        for q in tp:
            if len(q.shape) == 2:
                wee.append(q[alength,:])
            elif len(q.shape) == 1:
                wee.append(q[alength])
        app.append(wee)     
    return app
def makeClassifier(self,make_Fake, fully_connected_layers = None, conv_layers = None, dropout = 0.5, optimizer = &#39;adam&#39;, loss=&#39;binary_crossentropy&#39;):
    self.fully_connected_layers = fully_connected_layers
    self.dropout_p = dropout
    self.optimizer = optimizer
    self.loss = loss
    self.X = make_Fake[0]
    self.y = make_Fake[1]
    self.wind = make_Fake[2]
    self.fully_connected_layers = fully_connected_layers
    self.conv_layers = conv_layers
    input_size = self.X.shape[1]
    vocab_size = len(self.alphabet) + 1
    embedding_size = len(self.alphabet) + 1
    if self.fully_connected_layers == None:
        fully_connected_layers=[1024, 1024]
    if self.conv_layers == None:
        conv_layers = [[256, 7, 3],
               [256, 7, 3],
               [256, 3, -1],
               [256, 3, -1],
               [256, 3, -1],
               [256, 3, 3]]
            # Embedding weights
    embedding_weights = []  # (len(alphabet)+2, len(alphabet)+1)
    embedding_weights.append(np.zeros(vocab_size))  # (0, len(alphabet)+2)

    for char, i in self.wind.items():  # from index 1 to len(alphabet)+2
        onehot = np.zeros(vocab_size)
        onehot[i - 1] = 1
        embedding_weights.append(onehot)

    embedding_weights = np.array(embedding_weights)
    print(&#39;Load&#39;)

    # Embedding layer Initialization
    embedding_layer = Embedding(vocab_size + 1,
                                embedding_size,
                                input_length=input_size,
                                weights=[embedding_weights])

    # Model Construction
    # Input
    inputs = Input(shape=(input_size,), name=&#39;input&#39;, dtype=&#39;int64&#39;)         # shape=(?, padded input size)
    # Embedding
    x = embedding_layer(inputs)
    # Conv
    for filter_num, filter_size, pooling_size in conv_layers:
        x = Conv1D(filter_num, filter_size)(x)
        x = Activation(&#39;relu&#39;)(x)
        if pooling_size != -1:
            x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)
    x = Flatten()(x)  # (None, 8704)
    # Fully connected layers
    for dense_size in fully_connected_layers:
        x = Dense(dense_size, activation=&#39;relu&#39;)(x)  # dense_size == 1024
        x = Dropout(self.dropout_p)(x)
    # Output Layer
    predictions = Dense(1, activation=&#39;sigmoid&#39;)(x)
    # Build model
    model = Model(inputs=inputs, outputs=predictions)
    model.compile(optimizer=optimizer, loss=loss,
    metrics=[&#39;accuracy&#39;])  # Adam, categorical_crossentropy
    model.summary()
    return model
    #plot_model(model, to_file=self.directory + &#39;model_plot.png&#39;, show_shapes=True, show_layer_names=True)
    </code></pre>
</div>
<div id="histograms" class="section level3 unnumbered">
<h3>Histograms</h3>
<pre><code>scrimblo = fakeNews(lowercase=True)
edastart = scrimblo.make_fakeNews(eda=True)
bseq = np.sqrt(np.arange(626))
for q in range(len(edastart)):
    if edastart[q][&#39;y&#39;].values[0]==1:
        lbl = &#39;Fake&#39;
        col = &#39;#007f7f7f&#39;
    else:
        lbl = &#39;real&#39;
        col = &#39;#7f007f7f&#39;
    plt.hist(np.ravel(edastart[q][&#39;title&#39;].apply(lambda x: len(x)).values),bins=15*bseq,color=col, label=lbl,log=True)
plt.legend()
plt.title(&#39;Histogram of Lengths of Real vs. Fake Title Length&#39;)
plt.savefig(&#39;/mnt/c/Users/thecu/Documents/R Projects/4270/charLevelCNN/eda/&#39; + &#39;titles_hist.png&#39;)
plt.clf()
for q in range(len(edastart)):
    if edastart[q][&#39;y&#39;].values[0]==1:
        lbl = &#39;Fake&#39;
        col = &#39;#007f7f7f&#39;
    else:
        lbl = &#39;real&#39;
        col = &#39;#7f007f7f&#39;
    plt.hist(np.ravel(edastart[q][&#39;text&#39;].apply(lambda x: len(x)).values),bins=2500*(25-bseq)[::-1],color=col, label=lbl,log=True)
plt.legend()
plt.title(&#39;Histogram of Lengths of Real vs. Fake Article Body Length&#39;)
plt.savefig(
&#39;/mnt/c/Users/thecu/Documents/R Projects/4270/charLevelCNN/eda/&#39; + &#39;bodies_hist.png&#39;)</code></pre>
</div>
<div id="models" class="section level3 unnumbered">
<h3>Models</h3>
<div id="model-4" class="section level4 unnumbered">
<h4>Model 4</h4>
<pre><code>    scrimblo_u = fakeNews(lowercase=False)
zabloing_uf = scrimblo_u.make_fakeNews(content=False,raw=False)
scrungus_ud = scrimblo_u.makeClassifier(zabloing_uf,fully_connected_layers=[256,256])
kablooey_uf = scrimblo_u.three_way_split(zabloing_uf)
myModel3 = scrimblo_u.get_fit(scrungus_ud,kablooey_uf, name=&#39;model_3&#39;,
          batch_size = 64,
          epochs = 40,
          verbose = 2,
          display = True,
          predict = True)
          </code></pre>
</div>
<div id="model-6" class="section level4 unnumbered">
<h4>Model 6</h4>
<pre><code>scrimblo_u = fakeNews(lowercase=False)
zabloing_uf = scrimblo_u.make_fakeNews(content=False,raw=False,min_title_len=30,max_title_len=96,x_wid=123)
scrungus_ud = scrimblo_u.makeClassifier(zabloing_uf,fully_connected_layers=[256,256])
kablooey_uf = scrimblo_u.three_way_split(zabloing_uf)
myModel5 = scrimblo_u.get_fit(scrungus_ud,kablooey_uf, name=&#39;model_5&#39;,
          batch_size = 64,
          epochs = 40,
          verbose = 2,
          display = True,
          predict = True)</code></pre>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="references.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
